# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

Я решил произвести замеры использованной памяти (инструмент — ruby-prof).
Время выполнения скрипт (на следующем наборе входных данных: [1, 10, 100, 1_000, 10_000, 100_000] строк).
Количество созданных объектов (сфокусировался на классах, символах, массивах и строках).

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:

Я немного изменил код, чтобы считывать не весь большой файл целиком, а только первые его x линий:
```ruby
def work(lines = nil)
    file_name = lines ? 'data_large.txt' : 'data.txt'
    file_lines = lines ? File.read(file_name).split("\n", lines) : File.read(file_name).split("\n")
    ...
```
Это позволило мне проводить замеры не только на тестовых данных, но и на большом объеме данных (например, на 10_000 строках).
Для сбора метрики и прогона программы, я добавил еще несколько тестов.

## Вникаем в детали системы, чтобы найти 20% точек роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался: глазами, опытом, и результатами снятия метрик.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Замеры времени выполнения программы показали, что независимо от того, сколько мы позже хотим обработать строк,
загрузка большого файла целиком занимает в среднем 6,5 секунд на моей машине. 

Решил начать с того, чтобы грузить файл не целиком, а ровно столько его строк, сколько нам понадобиться.
Как минимум это позволит быстрее проводить замеры при дальнейшей оптимизации.

В итоге теперь файл считывается в память не целиком, а только нужно количество первых его строк.
Что сократило время "разогрева" с 6 секунд до 0.000200.

### Ваша находка №2
Погонял замеры времени выполнения скрипта с таким набором данных: `[1, 10, 100, 1_000, 10_000, 100_000]`.
Получил интересный результат (в секундах):
```bash
bench_work	 0.000915	 0.001853	 0.001687	 0.040583	 1.515984	315.730440
```
Глядя на радикальный прирост тормозов на 100_000 строк, можно сделать вывод, что дело не в количество строк (прирост строк всего лишь 10 кратный),
а в использовании памяти и неэффективных алгоритмах. 

Подобрал значение, которое выполняется в среднем за 6 секунд (20_000 строк),
и решил взять этот параметр за исходную точку для дальнейших замеров памяти и производительности.

Результаты профилирования показали, что самое узкое место на данный момент приходится на операцию Array#select.
Общее кол-во памяти, отведенной на эти операции, составило 414 Mb (из 470 всего). Когда количество вызовов этой функции всего 3,046 (по количеству users в первых 20_000 строк).
То есть, не самая частая операция отъедает почти 89% памяти.

Путем рефакторинга удалось получить следующие показатели (сокращение времени на 100_000 строках более чем в 157 раз):

```bash
bench_work	 0.001317	 0.000652	 0.001504	 0.019504	 0.231620	 2.087732
```

На 20_000 строчках памяти же теперь используется всего ~17 Mb против 466 Mb ранее, а времени 0,3 секунды, против 6.

### Ваша находка №3
Следующий метод на очереди, который съедает почти 70% памяти (или 13 Mb), оказался Array#each в методе `collect_stats_from_users`
Там куча `Array#map`, повторных проходов по массиву пользователей, и все такое. Вынес все это в виде методов модели User, закэшировал значения.
Путем всяких прочих хитрых рефакторингов удалось снизить расход памяти на этих операциях до 7 Mb. Что почти в два раза меньше. Время тоже сократилось в два раза.
Но повторные запуски метрики показали, что рефакторинг привел к большому росту расхода памяти на постройку хэша и последующую его конвертацию в json для записи на диск.
Но это уже друга история.

P.S. Кстати, нашел бажок. В тестах этого кейса нет. Если у пользователя вообще нет ни одной сессии, то он попадает в статистику как:
```json
...
"usedIE": false,
"alwaysUsedChrome": true,
...
```

### Ваша находка №4
После всех манипуляций почему-то вырос расход памяти на конвертацию финального хэша в json. Аж до 17 Mb.

Методом тыка обнаружил, что все дело было в моей "оптимизации" даты. Я ее стал хранить в хэше не в виде строк, а виде объекта Date.
Видимо потом #to_json не умело как-то ее конвертил, отчего отожрал памяти.

Вернул обратно строку, теперь весь стек с 20_000 строками укладывается в ~12 Mb.
Есть еще куда оптимизировать.

### Ваша находка №5
При помощи утилиты профилирования и бенчмарка удалось найти очередное бутылочное горлышко. А точнее, сразу два.
Первое — парсинг даты в объект.
Второе — String#split создавал дополнительные объекты массивов, ел нехило памяти.

С первым решилось просто — убрал парсинг (исходим из того, что формат даты неизменный), сортировал строку.
Со вторым пришлось переписать split используя блок. Выглядит коряво, совсем не руби-стайл. Но зато эффективнее. 

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы:

Время выполнение изначальное, с параметрами `[1, 10, 100, 1_000, 10_000, 100_000]`:

```bash
bench_work	 0.000915	 0.001853	 0.001687	 0.040583	 1.515984	315.730440
```

Стало:
```bash
bench_work	 0.000581	 0.000303	 0.000716	 0.007148	 0.068577	 0.648381
```

Ускорение более чем в 487 раз!

Изначальный расход памяти с кол-вом строк 20_000 был 470 Mb.
Стал: 7.1 Mb.

Итого уменьшение расхода памяти в 67 раз.

Весь файл целиком теперь выполняется за 29 секунд (ранее это занимало бесконечно много).
Память расходуется в районе ~2 Gb. (Если я правильно понял эту цифру в отчете профилировщика — 2 042 553)

Код все еще не самый красивый, есть что поDRYить, как разбить на классы, и где пописать больше тестов. Но вроде как урок
был не про это, поэтому можно пока пропустить и оставить как есть.

## Защита от регресса производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы добавил тест на O(N) бенчамарк.

Как сделать тест на используемую память пока не нашел.

## Задание №2

### Находка #1
Поискал профилировщиком процессора "бутылочные горлышки". И не нашел! Во всяком случае топ 10 методов выполняются быстро, никто особо одеяло на себя не тянет:
```bash
Measure Mode: wall_time
Thread ID: 70233629105680
Fiber ID: 70233628940200
Total: 0.258503
Sort by: self_time

 %self      total      self      wait     child     calls  name
 24.04      0.062     0.062     0.000     0.000    20000   String#split
 10.62      0.027     0.027     0.000     0.000    16954   Array#include?
  6.66      0.035     0.017     0.000     0.018        1   JSON::Ext::Generator::GeneratorMethods::Hash#to_json
  4.99      0.013     0.013     0.000     0.000    50466   String#encode
  4.09      0.013     0.011     0.000     0.002     9138   Array#map
  4.01      0.205     0.010     0.000     0.194     3046   Enumerator::Yielder#<<
  3.85      0.064     0.010     0.000     0.054    16954   Object#parse_session
  3.51      0.070     0.009     0.000     0.061     6092   User#stats
  3.18      0.008     0.008     0.000     0.000        2   <Class::IO>#foreach
```

На первом месте стоит `split`. Не думаю, что можно от него легко избавиться. А вот насчет `include?` есть варианты.
Вот эта строчка в коде, проверяет что браузер уже есть в списке, и тогда его добавлять уже не надо (чтобы избежать дупликатов):

```ruby
unique_browsers = []

user.sessions_browsers.each do |browser|
  unique_browsers << browser unless unique_browsers.include?(browser)
end
```

Набросал два варианта для бенчмарка, с массивами и с Set. По идее, Set содержит только уникальные элементы, так что в него можно пихать без проверки.
Результаты поразили! Оказывается, работа с Set почти в два раза медленнее, чем с обычными массивами и `include?`.

```bash
Warming up --------------------------------------
 include? with Array    10.148k i/100ms
           using SET     6.154k i/100ms
Calculating -------------------------------------
 include? with Array    105.783k (± 1.9%) i/s -      1.066M in  10.076880s
           using SET     62.109k (± 1.8%) i/s -    621.554k in  10.010569s

Comparison:
 include? with Array:   105782.8 i/s
           using SET:    62109.3 i/s - 1.70x  slower
```

Ну что ж, сделать быстрее не получилось, зато узнал для себя этот факт :)

### Находка #2
Ок, тогда попробуем избавиться от `include?` другим способом. А именно — уберем эту проверку, а в конце просто будем вызывать
`Array#uniq`.

Результаты бенчмарка:

```bash
Warming up --------------------------------------
      Array#include?    10.454k i/100ms
          Array#uniq    14.564k i/100ms
Calculating -------------------------------------
      Array#include?    108.048k (± 1.7%) i/s -      1.087M in  10.065433s
          Array#uniq    150.199k (± 1.7%) i/s -      1.515M in  10.087141s

Comparison:
          Array#uniq:   150199.5 i/s
      Array#include?:   108047.6 i/s - 1.39x  slower
```

Отлично, `uniq` в конце делает выполнение работы скрипта в 1,4 раза быстрее. Кажется, это то, что нужно.

Рефакторим основной код, смотрим замеры:

```bash
Measure Mode: wall_time
Thread ID: 70340592246260
Fiber ID: 70340592079380
Total: 0.228059
Sort by: self_time

 %self      total      self      wait     child     calls  name
 28.27      0.064     0.064     0.000     0.000    20000   String#split
  6.36      0.033     0.015     0.000     0.019        1   JSON::Ext::Generator::GeneratorMethods::Hash#to_json
  5.21      0.012     0.012     0.000     0.000    50466   String#encode
  4.71      0.013     0.011     0.000     0.002     9138   Array#map
  4.27      0.180     0.010     0.000     0.170     3046   Enumerator::Yielder#<<
  4.23      0.066     0.010     0.000     0.057    16954   Object#parse_session
  3.98      0.075     0.009     0.000     0.066     6092   User#stats
  3.15      0.007     0.007     0.000     0.000    16954   String#delete
  2.87      0.007     0.007     0.000     0.000     3046   Array#all?
  2.64      0.075     0.006     0.000     0.069     6092   Array#map!
```

Ну отлично, `include?` исчез, программа работает быстрее. Давайте теперь узнаем, насколько:

Было:
```bash
bench_work	 0.000581	 0.000303	 0.000716	 0.007148	 0.068577	 0.648381
```

Стало:
```bash
bench_work	 0.000639	 0.000241	 0.000631	 0.004630	 0.061788	 0.569467
```
Ура! Где-то аж на 10-15% выросла производительность.

### Находка #3

Увы, моя версия руби не поддерживается rbspy. Устанавливать более старые версии не стал, так что скипаю этот профайлер.

```bash
micro@Micro ~/D/t/task-1> sudo rbspy record --pid 31378
Press Ctrl+C to stop
thread '<unnamed>' panicked at 'Ruby version not supported yet: 2.6.1. Please create a GitHub issue and we'll fix it!', src/core/initialize.rs:254:14
note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.
Wrote raw data to /Users/micro/.cache/rbspy/records/rbspy-2019-03-22-nnxg6ExwY8.raw.gz
Writing formatted output to /Users/micro/.cache/rbspy/records/rbspy-2019-03-22-7QoqVkPJF3.flamegraph.svg
ERROR: No stack counts found
```

### Находка #4

Несмотря на все ухищрения, приложение по-прежнему жрет кучу памяти, и работает достаточно долго (~30 секунд).
Единственный способ напрашивается — перестать билдить весь JSON целиком, разбить процесс на мини блоки и обрабатывать их по очереди.

Переписал приложение по этой схеме. Теперь файл пишется не целиком за раз, а по мере обработки статистики, мелкими блоками, юзер за юзером.
Вся общая статистика дописывается в конце, закрывая JSON отдельной скобкой.

Также были применены две новые либы — OJ и Set. Первая очень быстро создает нужный JSON. Вторая гарантирует уникальность браузеров и их сортировку (SortedSet).

Итого, по скорости выполнения, было ([1, 10, 100, 1_000, 10_000, 100_000]):
```bash
bench_work	 0.000639	 0.000241	 0.000631	 0.004630	 0.061788	 0.569467
```

Стало
```bash
bench_work	 0.005764	 0.000386	 0.000655	 0.005837	 0.044759	 0.374959
```

Интересная особенность — на кол-ве линий 1, 10, 100 и даже 1_000 новая архитектура проиграла в скорости.
Зато на 10_000 и 100_000 прирост производительности составил 22%!

Общее время выполнения всего файла снизилась с 30 секунд до 12 секунд (более чем в два раза!).

Память с прежних 2 Гб теперь расходуется всего 40 мб (32-37). В 50 раз меньше!

Также добавил тестов, чтобы в дальнейшем было легче оптимизировать отдельные модули.
